# ===========================================
# Co-Op RAG Service Configuration
# ===========================================
# Copy this file to .env and fill in your values
#
# REQUIRED variables are marked with [REQUIRED]
# This service handles vector search for Legal/Finance agents

# ===========================================
# Server Configuration
# ===========================================
PORT=8000
HOST=0.0.0.0

# ===========================================
# Database [REQUIRED]
# Neon PostgreSQL - same database as backend
# Get from: https://console.neon.tech
# ===========================================
DATABASE_URL="postgresql://user:password@ep-xxx.region.aws.neon.tech/dbname?sslmode=require"

# ===========================================
# Supabase Storage [REQUIRED]
# For accessing uploaded PDF documents
# Get from: https://supabase.com/dashboard/project/[PROJECT]/settings/api
# ===========================================
SUPABASE_URL="https://your-project.supabase.co"
SUPABASE_SERVICE_KEY="eyJ..."
SUPABASE_STORAGE_BUCKET="documents"

# ===========================================
# Upstash Vector [REQUIRED]
# Vector database for semantic search
# Create index with: 768 dimensions, Cosine distance
# Get from: https://console.upstash.com/vector
# ===========================================
UPSTASH_VECTOR_REST_URL="https://your-index.upstash.io"
UPSTASH_VECTOR_REST_TOKEN="your-token"

# ===========================================
# Google AI [REQUIRED]
# For text embeddings (text-embedding-004, 768 dimensions)
# NOTE: This is for embeddings ONLY - LLM answer generation
# is handled by the backend's LLM Council
# Get from: https://aistudio.google.com/app/apikey
# ===========================================
GOOGLE_AI_API_KEY="AI..."

# ===========================================
# Security [REQUIRED]
# API key for backend authentication
# Must match RAG_API_KEY in backend .env
# ===========================================
RAG_API_KEY="your-secure-api-key"

# ===========================================
# CORS Configuration
# Backend URL(s) that can access this service
# ===========================================
CORS_ORIGINS="http://localhost:3000,https://your-backend.onrender.com"

# ===========================================
# Compression Provider (Optional)
# Uses HuggingFace Inference API - no local model download
# Compresses retrieved context before sending to LLM
# ===========================================
COMPRESSION_ENABLED="false"
COMPRESSION_PROVIDER="huggingface"
COMPRESSION_MODEL="mistralai/Mistral-7B-Instruct-v0.2"
HUGGINGFACE_API_KEY="hf_..."
