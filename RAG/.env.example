# ===========================================
# Co-Op RAG Service Environment Configuration
# ===========================================
# Copy this file to .env and fill in your values

# ===========================================
# Database (Neon PostgreSQL)
# Get from: https://console.neon.tech
# ===========================================
DATABASE_URL="postgres://user:password@host/database?sslmode=require"

# ===========================================
# Supabase (Storage Access)
# Get from: https://supabase.com/dashboard
# Need service key for storage access
# ===========================================
SUPABASE_URL="https://your-project.supabase.co"
SUPABASE_SERVICE_KEY="eyJ..."
SUPABASE_STORAGE_BUCKET="documents"

# ===========================================
# Upstash Vector
# Get from: https://console.upstash.com
# IMPORTANT: Create index with 768 dimensions (Gemini embedding size)
# Distance metric: Cosine
# ===========================================
UPSTASH_VECTOR_REST_URL="https://your-index.upstash.io"
UPSTASH_VECTOR_REST_TOKEN="your-token"

# ===========================================
# Google AI (Embeddings Only - NO LLM)
# Get from: https://aistudio.google.com/app/apikey
# Used for: text-embedding-004 (768 dimensions)
# 
# NOTE: LLM answer generation is handled by the backend's
# LLM Council. The RAG service only returns context chunks.
# ===========================================
GOOGLE_AI_API_KEY="your-google-ai-key"

# ===========================================
# Security
# API key for authenticating requests from backend
# Generate a secure random string
# ===========================================
RAG_API_KEY="your-secure-api-key"

# ===========================================
# CORS (Optional)
# Comma-separated list of allowed origins
# Leave empty to allow all origins (dev mode)
# ===========================================
CORS_ORIGINS="https://your-backend.onrender.com"

# ===========================================
# CLaRA - Semantic Document Compression (Optional)
# Apple CLaRa-7B-Instruct for query-aware context extraction
# Requires: GPU with CUDA, transformers, torch, accelerate
# ===========================================
# Set to "true" to enable CLaRA (requires GPU)
CLARA_ENABLED="false"
# Model to use (default: apple/CLaRa-7B-Instruct)
CLARA_MODEL="apple/CLaRa-7B-Instruct"
