# ===========================================
# Co-Op Backend Environment Configuration
# ===========================================

# Server
NODE_ENV=development
PORT=3000
API_PREFIX=api/v1

# Database (PostgreSQL via Supabase)
DATABASE_URL=postgresql://postgres:[PASSWORD]@db.[PROJECT].supabase.co:5432/postgres

# Supabase
SUPABASE_URL=https://[PROJECT].supabase.co
SUPABASE_ANON_KEY=your-anon-key
SUPABASE_SERVICE_KEY=your-service-key
SUPABASE_STORAGE_BUCKET=documents

# ===========================================
# Upstash Redis (single source for caching + queues)
# Get from: https://console.upstash.com
# ===========================================
UPSTASH_REDIS_URL=rediss://default:TOKEN@HOST.upstash.io:6379
UPSTASH_REDIS_TOKEN=your-upstash-token

# ===========================================
# LLM Providers (configure at least one)
# All providers are FREE tier compatible
# ===========================================

# Groq - Fast inference (https://console.groq.com)
# Models: llama-3.3-70b-versatile, mixtral-8x7b-32768, gemma2-9b-it
GROQ_API_KEY=gsk_your_groq_api_key

# Google AI Studio - Gemini (https://aistudio.google.com)
# Model: gemini-2.0-flash (latest)
GOOGLE_AI_API_KEY=your_google_ai_api_key

# HuggingFace Inference (https://huggingface.co/settings/tokens)
# Models: Mistral Small 24B, Qwen 2.5 72B, DeepSeek R1 32B
HUGGINGFACE_API_KEY=hf_your_huggingface_token

# ===========================================
# LLM Council Configuration
# Multiple models generate responses anonymously,
# then cross-critique each other for accuracy
# ===========================================
LLM_COUNCIL_MIN_MODELS=3
LLM_COUNCIL_MAX_MODELS=5

# MCP Integration (Optional)
MCP_ENDPOINT=http://localhost:8080
MCP_API_KEY=your-mcp-api-key

# Rate Limiting
THROTTLE_TTL=60
THROTTLE_LIMIT=100

# CORS
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# Logging
LOG_LEVEL=debug
